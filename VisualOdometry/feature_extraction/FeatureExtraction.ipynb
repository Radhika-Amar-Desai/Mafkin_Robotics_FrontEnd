{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import shutil\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "resnet_model.eval()\n",
    "resnet_model = torch.nn.Sequential(*(list(resnet_model.children())[:-1]))\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax ( x : float ) -> float: \n",
    "    \"\"\"\n",
    "        Returns output after applying softmax function on input x. \n",
    "    \"\"\"\n",
    "    return np.exp ( x ) / np.sum ( np.exp ( x ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_feat_vec ( image_path : str, \n",
    "                  transform = transform,\n",
    "                  model = resnet_model):\n",
    "    \"\"\"\n",
    "        Returns a feature vector for the image stored \n",
    "        at image path using pretrained model.\n",
    "    \"\"\"\n",
    "    image = Image.open ( image_path )\n",
    "    input_image = transform(image).unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    feat_vec_as_np_array = output.squeeze().numpy()\n",
    "    normalized_feat_vec_as_np_array = softmax ( feat_vec_as_np_array )\n",
    "    return normalized_feat_vec_as_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity_between_vectors ( first_feat_vec : list,\n",
    "                                        second_feat_vec : list ) -> float:\n",
    "    \"\"\"\n",
    "        Returns cosine similarity score between two feature vectors.\n",
    "    \"\"\"\n",
    "    return 1 - spatial.distance.cosine( first_feat_vec, second_feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance ( first_feat_vec : list, \n",
    "                            second_feat_vec : list ) -> float:\n",
    "    \"\"\"\n",
    "        Returns euclidean distance between two feature vectors.\n",
    "    \"\"\"\n",
    "    return spatial.distance.euclidean ( first_feat_vec, second_feat_vec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity_score ( first_image_path : str , second_image_path : str,\n",
    "                                transform = transform , model = resnet_model ) -> float:\n",
    "    \"\"\"\n",
    "        Returns cosine similarity score between two images based \n",
    "        on their feature vectors obatined from pretrained models.\n",
    "    \"\"\"    \n",
    "    first_image_feat_vec = get_normalized_feat_vec ( first_image_path , transform , model )\n",
    "    second_image_feat_vec = get_normalized_feat_vec ( second_image_path , transform , model )\n",
    "\n",
    "    return get_cosine_similarity_between_vectors ( first_image_feat_vec, \n",
    "                                                  second_image_feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_dissimilarity_score ( first_image_path : str , second_image_path : str ,\n",
    "                            transform = transform , model = resnet_model ) -> float:\n",
    "    \"\"\"\n",
    "        Returns dissimilarity score between two images based on the euclidean distance \n",
    "        between their feature vectors obatined from pretrained models.\n",
    "    \"\"\"    \n",
    "    first_image_feat_vec = get_normalized_feat_vec ( first_image_path , transform , model )\n",
    "    second_image_feat_vec = get_normalized_feat_vec ( second_image_path ,transform , model )\n",
    "\n",
    "    return get_euclidean_distance ( first_image_feat_vec , second_image_feat_vec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = r\"C:\\Users\\97433\\Mafkin_Robotics_FrontEnd\\VisualOdometry\\feature_extraction\\dataset\\blobs\\image_pair1\\blob0\\image2\\blob_at_2448_1924_image_2.jpg\"\n",
    "image2 = r\"C:\\Users\\97433\\Mafkin_Robotics_FrontEnd\\VisualOdometry\\feature_extraction\\dataset\\blobs\\image_pair1\\blob0\\image1\\blob_at_1573_2187_image_1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image ( image : list, border_len = 1 ):\n",
    "    \"\"\"\n",
    "        Pads image with 0s such that length and height\n",
    "        of image increase by border_len.\n",
    "    \"\"\"\n",
    "    return np.pad ( image , border_len, mode = \"constant\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_vec( data : list, min_val : int, max_val : int) -> list:\n",
    "    \"\"\"\n",
    "    Perform Min-Max normalization on the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A numpy array or list containing the data to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data: The normalized data.\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize ( x : int, min_val : int, max_val : int ) -> int:\n",
    "    return ( x - min_val ) / ( max_val - min_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob(image : list, key_point : tuple , blob_size : tuple):\n",
    "    \"\"\"\n",
    "    Extracts a patch (blob) from the image centered at the key_point\n",
    "    with the specified blob_size (height, width).\n",
    "\n",
    "    Args:\n",
    "    - image: The input image.\n",
    "    - key_point: The key point (x, y) around which the blob is extracted.\n",
    "    - blob_size: The size of the blob in pixels (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - blob_patch: The extracted patch (blob) from the image.\n",
    "    \"\"\"\n",
    "\n",
    "    kp_x, kp_y = int(key_point[0]), int(key_point[1])\n",
    "\n",
    "    blob_height, blob_width = blob_size\n",
    "\n",
    "    top_left_x = max(0, kp_x - blob_width // 2)\n",
    "    top_left_y = max(0, kp_y - blob_height // 2)\n",
    "\n",
    "    bottom_right_x = min(image.shape[1], kp_x + blob_width // 2)\n",
    "    bottom_right_y = min(image.shape[0], kp_y + blob_height // 2)\n",
    "\n",
    "    blob_patch = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "\n",
    "    return blob_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_sequence_at_point ( image : list, coord : tuple, \n",
    "                               blob_sizes = [ 10, 25, 50, 100 ] ) -> list:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - image (list): Image from which squence of blobs is to be generated.\n",
    "    - coord (tuple): A tuple containing the (x, y) coordinate of the center of each blob.\n",
    "    - blob_sizes (list): Sizes of blobs of the sequence.\n",
    "    Returns:\n",
    "    - blob_sequence (list): A list containing the blobs.\n",
    "                            Each blob is represented as a square patch\n",
    "                            of the specified size with its center\n",
    "                            as given by the center_coordinates.\n",
    "    \"\"\"\n",
    "    return [ get_blob ( image, coord, ( blob_size, blob_size)) \\\n",
    "            for blob_size in blob_sizes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_keypoints( first_image_path : list, second_image_path : list) -> list:\n",
    "    \"\"\" \n",
    "        Returns list of corresponding keypoints in both images sing ORB_FLANN feature matching.\n",
    "    \"\"\" \n",
    "    first_image = cv2.imread ( first_image_path , 0 )\n",
    "    second_image = cv2.imread ( second_image_path , 0 )\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    first_image_keypoints, first_descriptors = orb.detectAndCompute( first_image , None)\n",
    "    second_image_keypoints, second_descriptors = orb.detectAndCompute( second_image , None)\n",
    "\n",
    "    FLANN_INDEX_LSH = 6\n",
    "    index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n",
    "    search_params = dict(checks=50)  \n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch( first_descriptors, second_descriptors, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for match_pair in matches:\n",
    "        if len(match_pair) < 2:\n",
    "            continue\n",
    "        m , n = match_pair\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    first_image_matched_keypoints = [ (int(first_image_keypoints[match.queryIdx].pt[0]) ,\n",
    "                                      int(first_image_keypoints[match.queryIdx].pt[1]))  \\\n",
    "                                    for match in good_matches]\n",
    "    second_image_matched_keypoints = [ (int(second_image_keypoints[match.trainIdx].pt[0]),\n",
    "                                        int(second_image_keypoints[match.trainIdx].pt[1])) \\\n",
    "                                      for match in good_matches]\n",
    "\n",
    "    return [ [keypoint, second_image_matched_keypoints[index] ] \\\n",
    "            for index,keypoint in enumerate ( first_image_matched_keypoints )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_matching_keypoints ( first_image_path : str, second_image_path : str ) -> list:\n",
    "    \"\"\"\n",
    "        Returns corresponding keypoints between two images.\n",
    "    \"\"\"\n",
    "    orb_keypoints = get_matching_keypoints ( first_image_path, second_image_path )\n",
    "    return [ [ first_image_keypoint[ 0 ], orb_keypoints [::-1] [ index ][ 1 ]] \\\n",
    "            for index, first_image_keypoint in enumerate ( orb_keypoints ) \n",
    "            if first_image_keypoint[ 1 ] != orb_keypoints [ ::-1 ][ index ][ 1 ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matching_keypoints_blob_sequences ( folder_to_save : str, \n",
    "                                            image_pair_folder : str,\n",
    "                                            generate_keypoints_func : str,\n",
    "                                            generate_blob_sequence_func = get_blob_sequence_at_point,\n",
    "                                            extended_name_of_blob_sequence_folder = None) -> None:\n",
    "    \"\"\"\n",
    "        Functionality : \n",
    "        - Saves blob sequence consisting of blobs of various sizes having keypoints as center\n",
    "        for both images in the specified image pair folder. \n",
    "        \n",
    "        Parameters : \n",
    "        - folder_to_save : folder path to save blob_sequence.\n",
    "        - image_pair_folder : folder path for image pair.\n",
    "        - generate_keypoints_function : function used to generate keypoints.\n",
    "        - generate_blob_sequence_func : function used to generate blob sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.join ( folder_to_save ): os.makedirs ( folder_to_save )\n",
    "\n",
    "    first_image_path, second_image_path =\\\n",
    "        [ os.path.join ( image_pair_folder, image )\\\n",
    "        for image in os.listdir( image_pair_folder ) ]\n",
    "    \n",
    "    list_of_keypoints = generate_keypoints_func ( first_image_path, second_image_path )\n",
    "\n",
    "    first_image = cv2.imread ( first_image_path )\n",
    "    second_image = cv2.imread ( second_image_path )\n",
    "\n",
    "    blob_sequences_for_first_image = [ generate_blob_sequence_func ( first_image, keypoint[0] )\\\n",
    "                                      for keypoint in list_of_keypoints ]\n",
    "    \n",
    "    blob_sequences_for_second_image = [ generate_blob_sequence_func ( second_image, keypoint[1] )\\\n",
    "                                      for keypoint in list_of_keypoints ]\n",
    "\n",
    "    for index, blob_sequence in enumerate ( blob_sequences_for_first_image ):\n",
    "        blob_sequence_folder_name = \"blob_sequence_\" + str ( index ) + extended_name_of_blob_sequence_folder\n",
    "        blob_sequence_folder_path = os.path.join ( folder_to_save, blob_sequence_folder_name )\n",
    "        \n",
    "        if not os.path.exists ( blob_sequence_folder_path ): os.makedirs ( blob_sequence_folder_path )\n",
    "\n",
    "        first_image_blob_sequence_folder_path = os.path.join ( blob_sequence_folder_path, \"image1\" )\n",
    "        second_image_blob_sequence_folder_path = os.path.join ( blob_sequence_folder_path, \"image2\" )\n",
    "\n",
    "        if not os.path.exists ( first_image_blob_sequence_folder_path ): \n",
    "            os.makedirs ( first_image_blob_sequence_folder_path )\n",
    "\n",
    "        if not os.path.exists ( second_image_blob_sequence_folder_path ):\n",
    "            os.makedirs ( second_image_blob_sequence_folder_path )\n",
    "\n",
    "        for blob_index, blob_image in enumerate ( blob_sequence ):\n",
    "\n",
    "            blob_image_name = \"blob\" + str(blob_index) + \".jpg\"\n",
    "            \n",
    "            blob_image_path = os.path.join ( first_image_blob_sequence_folder_path, \n",
    "                                                        blob_image_name )\n",
    "            cv2.imwrite ( blob_image_path, blob_image )\n",
    "\n",
    "    \n",
    "        for blob_index, blob_image in enumerate ( blob_sequences_for_second_image [ index ] ):\n",
    "\n",
    "            blob_image_name = \"blob\" + str(blob_index) + \".jpg\"\n",
    "            \n",
    "            blob_image_path = os.path.join ( second_image_blob_sequence_folder_path, \n",
    "                                                        blob_image_name )\n",
    "            cv2.imwrite ( blob_image_path, blob_image )\n",
    "    print ( \"Done :)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_save_matching_keypoints_blob_sequences ( folder_to_save : str, \n",
    "                                            folder_containing_image_pairs : str,\n",
    "                                            generate_keypoints_func : str,\n",
    "                                            generate_blob_sequence_func = get_blob_sequence_at_point ) -> None:\n",
    "    \"\"\"\n",
    "        Functionality : \n",
    "        - Saves blob sequence consisting of blobs of various sizes having specified keypoints as center\n",
    "        for images of image pairs in the specified folder. \n",
    "        \n",
    "        Parameters : \n",
    "        - folder_to_save : folder path to save blob_sequence.\n",
    "        - folder_containing_image_pairs : folder path containing image pairs.\n",
    "        - generate_keypoints_func : function used to generate keypoints.\n",
    "        - generate_blob_sequence_func : function used to generate blob sequence.\n",
    "    \"\"\"\n",
    "    if not os.path.exists ( folder_to_save ): os.makedirs ( folder_to_save )\n",
    "\n",
    "    for index, image_pair_folder in enumerate ( os.listdir ( folder_containing_image_pairs ) ):\n",
    "        \n",
    "        image_pair_folder_path = os.path.join ( folder_containing_image_pairs, image_pair_folder )\n",
    "        \n",
    "        # image_pair_folder_to_save_blob_sequence = \"image_pair\" + str(index)\n",
    "        \n",
    "        # image_pair_folder_to_save_blob_sequence_path = os.path.join ( folder_to_save,\n",
    "        #                                                             image_pair_folder_to_save_blob_sequence )\n",
    "        \n",
    "        # if not os.path.exists ( image_pair_folder_to_save_blob_sequence_path ):\n",
    "        #     os.makedirs ( image_pair_folder_to_save_blob_sequence_path )\n",
    "\n",
    "        extended_name_of_blob_sequence_folder = \"_image_pair\" + str(index)\n",
    "\n",
    "        save_matching_keypoints_blob_sequences ( folder_to_save = folder_to_save,\n",
    "                                                image_pair_folder = image_pair_folder_path,\n",
    "                                                generate_blob_sequence_func = generate_blob_sequence_func,\n",
    "                                                generate_keypoints_func = generate_keypoints_func,\n",
    "                                                extended_name_of_blob_sequence_folder = \\\n",
    "                                                    extended_name_of_blob_sequence_folder)\n",
    "\n",
    "    print ( \"Done :)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset ( root_dir : str ) -> None:\n",
    "    \"\"\" \n",
    "        Balances dataset consisting of two classes by : \n",
    "        - removing additional files from class containing maximum number of files if difference \n",
    "        between number of files in both classes is less than 10. \n",
    "    \"\"\"\n",
    "    class_folder_paths = [ os.path.join ( root_dir, class_folder_name ) \\\n",
    "                          for class_folder_name in os.listdir ( root_dir ) ]\n",
    "    \n",
    "    class_folder_and_number_of_files = { class_folder_path : len(os.listdir( class_folder_path ))\\\n",
    "                                        for class_folder_path in class_folder_paths }\n",
    "    \n",
    "    sorted_class_folder_list = sorted ( class_folder_and_number_of_files, \n",
    "                                       key = class_folder_and_number_of_files.get )\n",
    "    \n",
    "    diff_in_number_of_files_in_classes = len ( os.listdir ( sorted_class_folder_list[1] ) ) - len ( os.listdir ( sorted_class_folder_list[0] ) )\n",
    "\n",
    "    if diff_in_number_of_files_in_classes < 20:\n",
    "        \n",
    "        majority_class_folder_path = sorted_class_folder_list [ 1 ]\n",
    "        \n",
    "        folders_in_majority_class =\\\n",
    "            [ os.path.join ( majority_class_folder_path, sub_folder ) \\\n",
    "            for sub_folder in os.listdir ( majority_class_folder_path )]\n",
    "    \n",
    "        folders_to_delete = random.sample(folders_in_majority_class, \n",
    "                                        min( diff_in_number_of_files_in_classes, \n",
    "                                            len(folders_in_majority_class)))\n",
    "\n",
    "        print ( len ( folders_to_delete ) )\n",
    "\n",
    "        for folder in folders_to_delete:\n",
    "            folder_path = os.path.join( majority_class_folder_path , folder)\n",
    "            shutil.rmtree ( folder_path )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_blob_sequence_dataset ( Dataset ):\n",
    "    \n",
    "    def __init__(self, root_dir : str ) -> None:\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([ transforms.Resize((224, 224)),\n",
    "                                            transforms.ToTensor() ])\n",
    "\n",
    "    def __len__ ( self ) -> int:\n",
    "        \n",
    "        class_folders = [ os.path.join ( self.root_dir, class_folder ) \\\n",
    "                        for class_folder in os.listdir ( self.root_dir ) ]\n",
    "        \n",
    "        return len ( os.listdir ( class_folders [ 0 ] ) ) + len ( os.listdir ( class_folders [ 1 ] ) )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        def get_images_from_image_folder_of_blob_sq ( blob_sq_folder : str, \n",
    "                                                    image_folder_index : int ):\n",
    "            \n",
    "            image_folder_name = [ os.path.join ( blob_sq_folder, image_folder ) \\\n",
    "                                for image_folder in os.listdir ( blob_sq_folder )][ image_folder_index ]\n",
    "            \n",
    "            image_paths =  [ os.path.join ( image_folder_name, image_path )\\\n",
    "                            for image_path in os.listdir ( image_folder_name ) ]\n",
    "            \n",
    "            transformed_images = [ self.transform(Image.open( image_path )) for image_path in image_paths ]\n",
    "            \n",
    "            return transformed_images\n",
    "\n",
    "        class_folders = [ os.path.join ( self.root_dir, class_folder ) \\\n",
    "                        for class_folder in os.listdir ( self.root_dir ) ]\n",
    "        \n",
    "        blob_sequences_paths = sorted([ os.path.join ( class_folder, blob_sequence_folder ) \\\n",
    "                            for class_folder in class_folders \\\n",
    "                            for blob_sequence_folder in os.listdir ( class_folder ) ])\n",
    "\n",
    "        labels = [ 1 if \"dissimilar\" == blob_sequence.split(\"\\\\\")[-2] else 0 \\\n",
    "                for blob_sequence in blob_sequences_paths  ]\n",
    "        \n",
    "        blob_sq_images = { image_folder_name : get_images_from_image_folder_of_blob_sq ( blob_sequences_paths[ index ], image_folder_index )\\\n",
    "                          for image_folder_index, image_folder_name in enumerate(os.listdir ( blob_sequences_paths [ index ] ))}\n",
    "\n",
    "        return blob_sq_images [ \"image1\" ], blob_sq_images [ \"image2\" ] , labels [ index ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes : int = 10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(weights = ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the fully connected layer\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Freeze the convolutional layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.resnet.conv1,\n",
    "                                        self.resnet.bn1,\n",
    "                                        self.resnet.relu,\n",
    "                                        self.resnet.maxpool,\n",
    "                                        self.resnet.layer1,\n",
    "                                        self.resnet.layer2,\n",
    "                                        self.resnet.layer3,\n",
    "                                        self.resnet.layer4,\n",
    "                                        self.resnet.avgpool)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential( nn.Linear ( 2048 * 4, 2048 * 2),\n",
    "                                        nn.Dropout ( 0.5 ),\n",
    "                                        nn.Linear ( 2048 * 2, 512 ),\n",
    "                                        nn.Dropout ( 0.5 ),\n",
    "                                        nn.Linear ( 512, 10 ))\n",
    "\n",
    "    def forward(self, x ):\n",
    "        \n",
    "        final_feat = torch.concat ( tuple( torch.flatten ( self.conv_layers ( x_comp ) ) \\\n",
    "                                    for x_comp in x ), dim = 0 )\n",
    "\n",
    "        return self.fc_layers(final_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn ( 1, 3, 224, 224 )\n",
    "model = CustomResNet()\n",
    "model ( [t,t,t,t] ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Define the architecture for each subnetwork (or branch)\n",
    "        self.subnetwork1 = CustomResNet(num_classes)\n",
    "        self.subnetwork2 = CustomResNet(num_classes)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass through both branches\n",
    "        output1 = self.subnetwork1(input1)\n",
    "        output2 = self.subnetwork2(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "      # Calculate the euclidian distance and calculate the contrastive loss\n",
    "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "\n",
    "      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "      return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( root_dir):\n",
    "    model = SiameseNetwork(10)\n",
    "\n",
    "    counter = []\n",
    "    loss_history = []\n",
    "    iteration_number= 0\n",
    "\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001 )\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    trainDS = custom_blob_sequence_dataset( root_dir)\n",
    "    train_dataloader = DataLoader ( trainDS, batch_size = 1, shuffle = False, pin_memory = True)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0.0\n",
    "        # Iterate over batches\n",
    "        for i, (blob_sq0, blob_sq1, label) in enumerate(train_dataloader):\n",
    "\n",
    "            # Send the images and labels to CUDA\n",
    "            #print ( img0 )\n",
    "            # img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
    "            blob_sq0 = [ blob.to(torch.device(DEVICE)) for blob in blob_sq0 ]\n",
    "            blob_sq1 = [ blob.to(torch.device(DEVICE)) for blob in blob_sq1 ]\n",
    "\n",
    "            blob_sq0 = sorted(blob_sq0, key=lambda x: x.shape)\n",
    "            blob_sq1 = sorted(blob_sq1, key=lambda x: x.shape)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass in the two images into the network and obtain two outputs\n",
    "            output1, output2 = model(blob_sq0, blob_sq1)\n",
    "            # Pass the outputs of the networks and label into the loss function\n",
    "            loss_contrastive = criterion(output1, output2, label)\n",
    "\n",
    "            epoch_loss += loss_contrastive.item()\n",
    "\n",
    "            # Calculate the backpropagation\n",
    "            loss_contrastive.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len ( train_dataloader )\n",
    "        loss_history.append ( avg_epoch_loss )\n",
    "        print(f\"Epoch [{epoch+1}/100], Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    print ( \"Training Completed.\" )\n",
    "    model_name = \"model\" \n",
    "    print ( \"Saving Model.\" )\n",
    "    torch.save ( model, model_name )\n",
    "    print ( \"Model saved.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m97433\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMafkin_Robotics_FrontEnd\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mVisualOdometry\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfeature_extraction\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdataset_for_ANN\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[119], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(root_dir)\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Pass in the two images into the network and obtain two outputs\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_sq0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblob_sq1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Pass the outputs of the networks and label into the loss function\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss_contrastive \u001b[38;5;241m=\u001b[39m criterion(output1, output2, label)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[100], line 11\u001b[0m, in \u001b[0;36mSiameseNetwork.forward\u001b[1;34m(self, input1, input2)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass through both branches\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubnetwork1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubnetwork2(input2)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output1, output2\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[98], line 34\u001b[0m, in \u001b[0;36mCustomResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x ):\n\u001b[1;32m---> 34\u001b[0m     final_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat ( \u001b[38;5;28mtuple\u001b[39m( torch\u001b[38;5;241m.\u001b[39mflatten ( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers ( x_comp ) ) \\\n\u001b[0;32m     35\u001b[0m                                 \u001b[38;5;28;01mfor\u001b[39;00m x_comp \u001b[38;5;129;01min\u001b[39;00m x ), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m )\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers(final_feat)\n",
      "Cell \u001b[1;32mIn[98], line 34\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x ):\n\u001b[1;32m---> 34\u001b[0m     final_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat ( \u001b[38;5;28mtuple\u001b[39m( torch\u001b[38;5;241m.\u001b[39mflatten ( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_comp\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m ) \\\n\u001b[0;32m     35\u001b[0m                                 \u001b[38;5;28;01mfor\u001b[39;00m x_comp \u001b[38;5;129;01min\u001b[39;00m x ), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m )\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers(final_feat)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\resnet.py:161\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m--> 161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1469\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train ( r\"C:\\Users\\97433\\Mafkin_Robotics_FrontEnd\\VisualOdometry\\feature_extraction\\dataset_for_ANN\\train\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
